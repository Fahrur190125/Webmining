{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQTMO+dv7kWTMWUwhkP2UF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Topik Modelling Dengan Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA) menggunakan Scikit-Learn\n","Dalam pembahasan kali ini, kita akan fokus pada Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA) dan melakukan topik modelling menggunakan Scikit-learn."],"metadata":{"id":"79C8eWKB1P4M"}},{"cell_type":"markdown","source":["## **Topik Modelling**\n","Topik Modelling ialah teknik tanpa pengawasan untuk menemukan tema dokumen yang diberikan. Ini mengekstrak kumpulan kata kunci yang terjadi bersama. Kata kunci yang muncul bersama ini mewakili sebuah topik. Misalnya, saham, pasar, ekuitas, reksa dana akan mewakili topik 'investasi saham'."],"metadata":{"id":"yxJVH7Ha23vO"}},{"cell_type":"markdown","source":["## **Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA)**\n","Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA)  adalah teknik dalam natural language processing , khususnya  distributional semantics , yang menganalisis hubungan antara satu set dokumen dan istilah yang dikandungnya dengan menghasilkan satu set konsep yang terkait dengan dokumen dan istilah. LSA mengasumsikan bahwa kata-kata yang memiliki makna yang dekat akan muncul dalam potongan teks yang serupa (  distributional hypothesis ). Sebuah matriks yang berisi jumlah kata per dokumen (baris mewakili kata-kata unik dan kolom mewakili setiap dokumen) dibangun dari sepotong besar teks dan teknik matematika yang disebut Singular Value Decomposition (SVD) digunakan untuk mengurangi jumlah baris dengan tetap menjaga kesamaan struktur antar kolom. Dokumen kemudian dibandingkan dengan mengambil kosinus sudut antara dua vektor (atau produk titik antara normalisasi dua vektor) yang dibentuk oleh dua kolom. Nilai yang mendekati 1 menunjukkan dokumen yang sangat mirip sedangkan nilai yang mendekati 0 menunjukkan dokumen yang sangat berbeda.<br>\n","<center><img src='https://media.geeksforgeeks.org/wp-content/uploads/20210406165951/Screenshot20210406165933.png'></center><center>Gambar LSA</center> Untuk melakukan LSA dapat dilakukan dengan mengikuti tahapan tahapan berikut."],"metadata":{"id":"ysFKklwn3F_H"}},{"cell_type":"markdown","source":["## **Mengambil Dokumen**\n","Langkah awal untuk melakukan Topik Modelling ialah dengan mengambil dokumen tersebut dengan mengcrawling data dokumen dengan menggunakan scrapy & crochet seperti berikut."],"metadata":{"id":"RXtSvKfs3oiP"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBDcPqq_nXi_","executionInfo":{"status":"ok","timestamp":1666651046771,"user_tz":-420,"elapsed":24201,"user":{"displayName":"sumina so","userId":"07390640577398086821"}},"outputId":"3640c6b3-7c95-4c7d-94c5-b0b475da7c8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scrapy\n","  Downloading Scrapy-2.7.0-py2.py3-none-any.whl (270 kB)\n","\u001b[K     |████████████████████████████████| 270 kB 8.6 MB/s \n","\u001b[?25hCollecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting Twisted>=18.9.0\n","  Downloading Twisted-22.8.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 51.2 MB/s \n","\u001b[?25hCollecting service-identity>=18.1.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting w3lib>=1.17.0\n","  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n","Collecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n","Collecting cryptography>=3.3\n","  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 51.9 MB/s \n","\u001b[?25hCollecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n","Collecting tldextract\n","  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 3.2 MB/s \n","\u001b[?25hCollecting cssselect>=0.9.1\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting pyOpenSSL>=21.0.0\n","  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n","\u001b[?25hCollecting zope.interface>=5.1.0\n","  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n","\u001b[K     |████████████████████████████████| 254 kB 76.4 MB/s \n","\u001b[?25hCollecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n","Collecting protego>=0.1.15\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n","Collecting parsel>=1.5.0\n","  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n","Building wheels for collected packages: PyDispatcher\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=c51d8f1db21b459dd2498a5580788eb8d2a48f6dbf90dc0ffdca4872c1660345\n","  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n","Successfully built PyDispatcher\n","Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n","Successfully installed Automat-20.2.0 PyDispatcher-2.0.6 Twisted-22.8.0 constantly-15.1.0 cryptography-38.0.1 cssselect-1.1.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.0 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.0.1 zope.interface-5.5.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting crochet\n","  Downloading crochet-2.0.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: Twisted>=16.0 in /usr/local/lib/python3.7/dist-packages (from crochet) (22.8.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from crochet) (1.14.1)\n","Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (5.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (4.1.1)\n","Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (22.10.0)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (15.1.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (22.1.0)\n","Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (20.2.0)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Automat>=0.8.0->Twisted>=16.0->crochet) (1.15.0)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (2.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet) (57.4.0)\n","Installing collected packages: crochet\n","Successfully installed crochet-2.0.0\n"]}],"source":["!pip install scrapy\n","!pip install crochet"]},{"cell_type":"code","source":["import scrapy\n","from scrapy.crawler import CrawlerRunner\n","import re\n","from crochet import setup, wait_for\n","setup()\n","\n","class QuotesToCsv(scrapy.Spider):\n","    name = \"MJKQuotesToCsv\"\n","    start_urls = [\n","        'https://tekno.tempo.co/read/1580340/peran-penting-iptekin-terhadap-kemajuan-sebuah-bangsa'\n","    ]\n","    custom_settings = {\n","        'ITEM_PIPELINES': {\n","            '__main__.ExtractFirstLine': 1\n","        },\n","        'FEEDS': {\n","            'news.csv': {\n","                'format': 'csv',\n","                'overwrite': True\n","            }\n","        }\n","    }\n","\n","    def parse(self, response):\n","        \"\"\"parse data from urls\"\"\"\n","        for quote in response.css('#isi > p'):\n","            yield {'news': quote.extract()}\n","\n","\n","class ExtractFirstLine(object):\n","    def process_item(self, item, spider):\n","        \"\"\"text processing\"\"\"\n","        lines = dict(item)[\"news\"].splitlines()\n","        first_line = self.__remove_html_tags__(lines[0])\n","\n","        return {'news': first_line}\n","\n","    def __remove_html_tags__(self, text):\n","        \"\"\"remove html tags from string\"\"\"\n","        html_tags = re.compile('<.*?>')\n","        return re.sub(html_tags, '', text)\n","\n","@wait_for(10)\n","def run_spider():\n","    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n","    crawler = CrawlerRunner()\n","    d = crawler.crawl(QuotesToCsv)\n","    return d"],"metadata":{"id":"MjU0dNBRndL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_spider()"],"metadata":{"id":"hLvP-t2ISp3c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666651049067,"user_tz":-420,"elapsed":1715,"user":{"displayName":"sumina so","userId":"07390640577398086821"}},"outputId":"bb6df657-a94d-432a-9199-811eb775aaad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:twisted:/usr/local/lib/python3.7/dist-packages/scrapy/utils/request.py:231: scrapy.exceptions.ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n","\n","It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n","\n","See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n"]}]},{"cell_type":"markdown","source":["## **Meload Dokumen**\n","Setelah tahapan mengambil dokumen selesai, selanjutnya meload dokumen yang sudah didapatkan. Untuk dapat meload dokumen kita gunakan library os dan pandas seperti berikut."],"metadata":{"id":"6jnEnUou4Nnc"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# Load Dataset\n","documents_list = []\n","with open( os.path.join(\"news.csv\") ,\"r\") as fin:\n","    for line in fin.readlines():\n","        text = line.strip()\n","        documents_list.append(text)"],"metadata":{"id":"4McryBFvteQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Membuat Fitur TF-IDF**\n","Setelah berhasil meload dokumen langkah selanjutnya ialah mengenerate fitur TF-IDF pada dokumen. Pada proses ini juga dilakukan operasi prepocessing, yaitu case folding, stopword, dan tokenizing. Untuk melakukan proses ini dengan menggunakan RegexpTokenizer dari library nltk seperti source code berikut."],"metadata":{"id":"epO6fJPE1B0s"}},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize regex tokenizer\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","# Vectorize document using TF-IDF\n","tfidf = TfidfVectorizer(lowercase=True,\n","                        stop_words='english',\n","                        ngram_range = (1,1),\n","                        tokenizer = tokenizer.tokenize)\n","\n","# Fit and Transform the documents\n","train_data = tfidf.fit_transform(documents_list)  \n","train_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hof42h0tz0S","executionInfo":{"status":"ok","timestamp":1666651050624,"user_tz":-420,"elapsed":1157,"user":{"displayName":"sumina so","userId":"07390640577398086821"}},"outputId":"b39b9fbb-c5ad-43f6-ba5f-6fab0e0acbf4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<12x236 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 412 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## **Membuat Matrik SVD**\n","Matrik SVD adalah teknik dekomposisi matriks yang memfaktorkan matriks dalam produk matriks. Untuk dapat membuat matrik tersebut kita dapat menggunakan TruncatedSVD dari library sklearn seperti berikut."],"metadata":{"id":"yw_YCNyu1O5_"}},{"cell_type":"code","source":["from sklearn.decomposition import TruncatedSVD\n","# Define the number of topics or components\n","num_components=12\n","\n","# Create SVD object\n","lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n","\n","# Fit SVD model on data\n","lsa.fit_transform(train_data)\n","\n","# Get Singular values and Components \n","Sigma = lsa.singular_values_  \n","V_transpose = lsa.components_.T\n","V_transpose"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QjAGqG9Ntnyv","executionInfo":{"status":"ok","timestamp":1666651050625,"user_tz":-420,"elapsed":21,"user":{"displayName":"sumina so","userId":"07390640577398086821"}},"outputId":"5ff72ca1-6732-483a-810c-d45490ac8c1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3.02366178e-02, -1.66382861e-02,  3.00062190e-02, ...,\n","         1.35817657e-01, -1.98807570e-03,  7.74902033e-03],\n","       [ 8.38985795e-02,  2.10631147e-01, -4.69070221e-02, ...,\n","         1.82148827e-02, -2.80810446e-02, -1.39858160e-01],\n","       [ 2.10849282e-02, -1.84815299e-02,  2.61033720e-03, ...,\n","        -3.35817930e-02, -4.12961820e-04, -4.39328074e-03],\n","       ...,\n","       [ 2.52767786e-02, -2.00135859e-02, -5.03940302e-02, ...,\n","        -3.07241369e-02,  1.88065252e-04, -2.37049787e-02],\n","       [ 1.58110182e-02, -2.27024106e-02,  5.80867655e-02, ...,\n","        -2.65079399e-02, -1.14091743e-02, -1.09058826e-02],\n","       [ 2.27840084e-01, -1.53073584e-01, -1.05864584e-03, ...,\n","        -4.25488190e-02, -4.07617741e-03,  7.17080829e-02]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## **Ekstrak topik dan istilah**\n","Setelah membuar matriks SVD, Selanjutnya kita perlu mengekstrak topik dari matriks komponen SVD dengan source code seperti berikut. "],"metadata":{"id":"4PUxh1u21SxN"}},{"cell_type":"code","source":["# Print the topics with their terms\n","terms = tfidf.get_feature_names()\n","\n","for index, component in enumerate(lsa.components_):\n","    zipped = zip(terms, component)\n","    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n","    top_terms_list=list(dict(top_terms_key).keys())\n","    print(\"Topic \"+str(index)+\": \",top_terms_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IJSROA_uH7T","executionInfo":{"status":"ok","timestamp":1666651050626,"user_tz":-420,"elapsed":17,"user":{"displayName":"sumina so","userId":"07390640577398086821"}},"outputId":"cdc418f0-a1e3-4b35-ffa1-eb1dd9fa30eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:twisted:/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: builtins.FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n"]},{"output_type":"stream","name":"stdout","text":["Topic 0:  ['negara', 'dan', 'yang', 'di', 'dalam']\n","Topic 1:  ['elemen', 'ilmu', 'kunci', 'pengetahuan', 'adalah']\n","Topic 2:  ['kemudian', 'juga', 'pendekatan', 'disebut', 'berbagai']\n","Topic 3:  ['news', 'akan', 'sistem', 'pendekatan', 'kemudian']\n","Topic 4:  ['bahan', 'korea', 'sangat', 'selatan', 'taiwan']\n","Topic 5:  ['hal', 'baik', 'ini', 'cendekiawan', 'diskursus']\n","Topic 6:  ['seringkali', 'banyak', 'sektor', 'cendekiawan', 'diskursus']\n","Topic 7:  ['dan', 'juga', 'nies', 'antaranya', 'kelembagaan']\n","Topic 8:  ['dokumen', 'bahan', 'korea', 'sangat', 'selatan']\n","Topic 9:  ['ekonomi', 'bagian', 'begitu', 'berupaya', 'catch']\n","Topic 10:  ['halnya', 'indonesia', 'langsung', 'menerapkan', 'mengabsorbsi']\n","Topic 11:  ['amerika', 'austria', 'bagi', 'bahasa', 'belanda']\n"]}]},{"cell_type":"markdown","source":["## **Kesimpulan**\n","Hasil yang didapatkan dari topik modelling dengan Latent Semantic Indexing (LSI) atau Latent Semantic Analysis menggunakan library scikit-learn dengan mengambil 12 topik sebagai berikut.<br>\n","Topic 1:  ['negara', 'dan', 'yang', 'di', 'dalam']<br>\n","Topic 2:  ['elemen', 'ilmu', 'kunci', 'pengetahuan', 'adalah']<br>\n","Topic 3:  ['kemudian', 'juga', 'pendekatan', 'disebut', 'berbagai']<br>\n","Topic 4:  ['news', 'akan', 'sistem', 'pendekatan', 'kemudian']<br>\n","Topic 5:  ['bahan', 'korea', 'sangat', 'selatan', 'taiwan']<br>\n","Topic 6:  ['hal', 'baik', 'ini', 'cendekiawan', 'diskursus']<br>\n","Topic 7:  ['seringkali', 'banyak', 'sektor', 'cendekiawan', 'diskursus']<br>\n","Topic 8:  ['dan', 'juga', 'nies', 'antaranya', 'kelembagaan']<br>\n","Topic 9:  ['dokumen', 'bahan', 'korea', 'sangat', 'selatan']<br>\n","Topic 10:  ['ekonomi', 'bagian', 'begitu', 'berupaya', 'catch']<br>\n","Topic 12:  ['halnya', 'indonesia', 'langsung', 'menerapkan', 'mengabsorbsi']<br>\n","Topic 12:  ['amerika', 'austria', 'bagi', 'bahasa', 'belanda']"],"metadata":{"id":"7_mDILoTHXYo"}}]}